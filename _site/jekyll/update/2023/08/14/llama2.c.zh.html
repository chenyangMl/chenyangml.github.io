<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>大语言模型——如何构建词表(Tokenizer) | Chen Yang’s Blog</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="大语言模型——如何构建词表(Tokenizer)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Modify vocabulary" />
<meta property="og:description" content="Modify vocabulary" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2023/08/14/llama2.c.zh.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2023/08/14/llama2.c.zh.html" />
<meta property="og:site_name" content="Chen Yang’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-14T14:48:11+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="大语言模型——如何构建词表(Tokenizer)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-08-14T14:48:11+08:00","datePublished":"2023-08-14T14:48:11+08:00","description":"Modify vocabulary","headline":"大语言模型——如何构建词表(Tokenizer)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2023/08/14/llama2.c.zh.html"},"url":"http://localhost:4000/jekyll/update/2023/08/14/llama2.c.zh.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Chen Yang's Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Chen Yang&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">大语言模型——如何构建词表(Tokenizer)</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-08-14T14:48:11+08:00" itemprop="datePublished">Aug 14, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><code class="language-plaintext highlighter-rouge">Modify vocabulary</code></p>

<p>As shown below, the original llama2-en mainly sample with english corpus, and llama2_ enzh forms a new 52k vocabulary by adding Chinese corpus statistics and merging the original vocabulary.</p>

<p>The number of tokens consumed by encoding a sentence s using the original llama2-en vocabulary is 57, but the expanded llama2_ enzh vocabulary encoding same sentence s only need 19 tokens. So, when the original vocabulary does not involve the current scene corpus, expanding the vocabulary may be a good choice.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">from</span> <span class="n">sentencepiece</span> <span class="n">import</span> <span class="no">SentencePieceProcessor</span>
<span class="n">sp_model_llama2</span> <span class="o">=</span> <span class="no">SentencePieceProcessor</span><span class="p">(</span><span class="n">model_file</span><span class="o">=</span><span class="s2">"tokenizers/llama2en/tokenizer.model"</span><span class="p">)</span>
<span class="n">sp_model_llamaenzh</span> <span class="o">=</span> <span class="no">SentencePieceProcessor</span><span class="p">(</span><span class="n">model_file</span><span class="o">=</span><span class="s2">"tokenizers/llama2enzh/tokenizer.model"</span><span class="p">)</span>
<span class="n">sp_model_baichaun</span> <span class="o">=</span> <span class="no">SentencePieceProcessor</span><span class="p">(</span><span class="n">model_file</span><span class="o">=</span><span class="s2">"tokenizers/baichuan/tokenizer.model"</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="s2">"从前，有一个小女孩，名叫莉莉。她喜欢在外面玩耍，欣赏美丽的花朵。"</span>

<span class="n">llama2_en</span> <span class="o">=</span> <span class="n">sp_model_llama2</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">baichuan_enzh</span> <span class="o">=</span> <span class="n">sp_model_baichaun</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">llama2_enzh</span> <span class="o">=</span> <span class="n">sp_model_llamaenzh</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"llama2_en={len(llama2_en)}, llama2_enzh={len(llama2_enzh)} baichuan-enzh={len(baichuan_enzh)}"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"llama2_en={sp_model_llama2.vocab_size()}, llama2_enzh={sp_model_llamaenzh.vocab_size()} \
      baichuan-enzh={sp_model_baichaun.vocab_size()}"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"llama2_en"</span><span class="p">,</span><span class="n">llama2_en</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"llama2_enzh"</span><span class="p">,</span><span class="n">llama2_enzh</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"#tokens used when encode a zh-token, llam2-en={len(llama2_en)/len(s):.2}, llama2_enzh={len(llama2_enzh)/len(s):.2}"</span><span class="p">)</span>
<span class="c1">#-----------------------------------</span>
<span class="n">llama2_en</span><span class="o">=</span><span class="mi">57</span><span class="p">,</span> <span class="n">llama2_enzh</span><span class="o">=</span><span class="mi">19</span> <span class="n">baichuan</span><span class="o">-</span><span class="n">enzh</span><span class="o">=</span><span class="mi">23</span>
<span class="n">llama2_en</span><span class="o">=</span><span class="mi">32000</span><span class="p">,</span> <span class="n">llama2_enzh</span><span class="o">=</span><span class="mi">55296</span>       <span class="n">baichuan</span><span class="o">-</span><span class="n">enzh</span><span class="o">=</span><span class="mi">64000</span>
<span class="n">llama2_en</span> <span class="p">[</span><span class="mi">29871</span><span class="p">,</span> <span class="mi">31594</span><span class="p">,</span> <span class="mi">30658</span><span class="p">,</span> <span class="mi">30214</span><span class="p">,</span> <span class="mi">30417</span><span class="p">,</span> <span class="mi">30287</span><span class="p">,</span> <span class="mi">30502</span><span class="p">,</span> <span class="mi">30446</span><span class="p">,</span> <span class="mi">30647</span><span class="p">,</span> <span class="mi">232</span><span class="p">,</span> <span class="mi">176</span><span class="p">,</span> <span class="mi">172</span><span class="p">,</span> <span class="mi">30214</span><span class="p">,</span> <span class="mi">30548</span><span class="p">,</span> <span class="mi">232</span><span class="p">,</span> <span class="mi">146</span><span class="p">,</span> <span class="mi">174</span><span class="p">,</span> <span class="mi">235</span><span class="p">,</span> <span class="mi">145</span><span class="p">,</span> <span class="mi">140</span><span class="p">,</span> <span class="mi">235</span><span class="p">,</span> <span class="mi">145</span><span class="p">,</span> <span class="mi">140</span><span class="p">,</span> <span class="mi">30267</span><span class="p">,</span> <span class="mi">232</span><span class="p">,</span> <span class="mi">168</span><span class="p">,</span> <span class="mi">188</span><span class="p">,</span> <span class="mi">31823</span><span class="p">,</span> <span class="mi">233</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mi">165</span><span class="p">,</span> <span class="mi">30505</span><span class="p">,</span> <span class="mi">31066</span><span class="p">,</span> <span class="mi">30806</span><span class="p">,</span> <span class="mi">234</span><span class="p">,</span> <span class="mi">145</span><span class="p">,</span> <span class="mi">172</span><span class="p">,</span> <span class="mi">235</span><span class="p">,</span> <span class="mi">131</span><span class="p">,</span> <span class="mi">144</span><span class="p">,</span> <span class="mi">30214</span><span class="p">,</span> <span class="mi">233</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mi">166</span><span class="p">,</span> <span class="mi">235</span><span class="p">,</span> <span class="mi">184</span><span class="p">,</span> <span class="mi">146</span><span class="p">,</span> <span class="mi">30630</span><span class="p">,</span> <span class="mi">231</span><span class="p">,</span> <span class="mi">187</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">30210</span><span class="p">,</span> <span class="mi">30830</span><span class="p">,</span> <span class="mi">233</span><span class="p">,</span> <span class="mi">159</span><span class="p">,</span> <span class="mi">184</span><span class="p">,</span> <span class="mi">30267</span><span class="p">]</span>
<span class="n">llama2_enzh</span> <span class="p">[</span><span class="mi">29871</span><span class="p">,</span> <span class="mi">40870</span><span class="p">,</span> <span class="mi">30214</span><span class="p">,</span> <span class="mi">32952</span><span class="p">,</span> <span class="mi">41677</span><span class="p">,</span> <span class="mi">30214</span><span class="p">,</span> <span class="mi">40148</span><span class="p">,</span> <span class="mi">34595</span><span class="p">,</span> <span class="mi">34595</span><span class="p">,</span> <span class="mi">30267</span><span class="p">,</span> <span class="mi">32008</span><span class="p">,</span> <span class="mi">32123</span><span class="p">,</span> <span class="mi">40729</span><span class="p">,</span> <span class="mi">42754</span><span class="p">,</span> <span class="mi">30214</span><span class="p">,</span> <span class="mi">35186</span><span class="p">,</span> <span class="mi">37973</span><span class="p">,</span> <span class="mi">46892</span><span class="p">,</span> <span class="mi">30267</span><span class="p">]</span>
<span class="c1">#tokens used when encode a zh-token, llam2-en=1.8, llama2_enzh=0.59</span></code></pre></figure>

<p>You can refer to the resources to prepare a customized vocabulary, and then start training from scratch.</p>

<p><strong>new vocab</strong>：https://huggingface.co/docs/tokenizers/pipeline</p>

<p><strong>extend vocab</strong>：<a href="https://github.com/google/sentencepiece/blob/9cf136582d9cce492ba5a0cfb775f9e777fe07ea/python/add_new_vocab.ipynb">sentencepiece add new vocab</a></p>

<p><strong>reduce vocan</strong>：</p>
<ul>
  <li>
    <p><a href="https://blog.ceshine.net/post/trim-down-sentencepiece-vocabulary/">Reducing the SentencePiece Vocabulary Size of Pretrained NLP Models</a></p>
  </li>
  <li>
    <p><a href="https://github.com/bojone/t5_in_bert4keras/blob/6cf50dbf3ffd3b4e9f36a59ee9f98356cf686de0/tokenizer/reduce.py">toknizer reduce</a></p>
  </li>
</ul>


  </div><a class="u-url" href="/jekyll/update/2023/08/14/llama2.c.zh.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Chen Yang&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Chen Yang&#39;s Blog</li><li><a class="u-email" href="mailto:cyang5080@gmail.com">cyang5080@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/chenyangMl"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">chenyangMl</span></a></li><li><a href="https://www.twitter.com/cyang8050"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">cyang8050</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Machine learning Engineer. Focusing on computer vision and natural language processing.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
